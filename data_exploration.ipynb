{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os, glob\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 01:16:59 WARN Utils: Your hostname, m-hassib resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface wlp3s0)\n",
      "23/04/15 01:16:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/m-hassib/.ivy2/cache\n",
      "The jars for the packages stored in: /home/m-hassib/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-163116d5-4660-4e7e-a651-c0e86857ce62;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\t[2.12.5] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 9422ms :: artifacts dl 234ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-163116d5-4660-4e7e-a651-c0e86857ce62\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/136ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 01:17:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\n",
    "        \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    "        ).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope the Project and Gather Data\n",
    "For this project, I have decided to gather data from two sources: the New York City Taxi and Limousine Commission's (TLC) trip record data and weather data \n",
    "scraped from https://www.wunderground.com/.  \n",
    "\n",
    "The end use case for this data is to create an analytics table that can be used to analyze the relationship between weather conditions and trips in New York City.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TLC data \n",
    "provides information on taxi and for-hire vehicle trips in New York City, including pickup and dropoff \n",
    "times and locations, fares, and payment types in this project we will work on for-hire vehicle trips data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Trips data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee| tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:18:06|2023-01-01 02:19:24|2023-01-01 02:19:38|2023-01-01 02:48:07|          48|          68|      0.94|     1709|              25.95|  0.0|0.78|      2.3|                2.75|        0.0| 5.22|     27.83|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:48:42|2023-01-01 02:56:20|2023-01-01 02:58:39|2023-01-01 03:33:08|         246|         163|      2.78|     2069|              60.14|  0.0| 1.8|     5.34|                2.75|        0.0|  0.0|     50.15|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:15:35|2023-01-01 02:20:14|2023-01-01 02:20:27|2023-01-01 02:37:54|           9|         129|      8.81|     1047|              24.37|  0.0|0.73|     2.16|                 0.0|        0.0|  0.0|     20.22|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:35:24|2023-01-01 02:39:30|2023-01-01 02:41:05|2023-01-01 02:48:16|         129|         129|      0.67|      431|               13.8|  0.0|0.41|     1.22|                 0.0|        0.0|  0.0|       7.9|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:43:15|2023-01-01 02:51:10|2023-01-01 02:52:47|2023-01-01 03:04:51|         129|          92|      4.38|      724|              20.49|  0.0|0.61|     1.82|                 0.0|        0.0|  0.0|     16.48|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 02:21:34|               null|2023-01-01 02:29:05|2023-01-01 02:49:54|         130|          38|     4.921|     1249|              18.29|  0.0|0.43|     1.27|                 0.0|        0.0|  0.0|     16.81|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 02:47:17|               null|2023-01-01 02:55:29|2023-01-01 03:16:07|          38|          10|     5.517|     1238|              25.76|  0.0|0.77|     2.29|                 0.0|        0.0|  0.0|     23.65|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:06:54|2023-01-01 02:08:59|2023-01-01 02:10:29|2023-01-01 02:18:22|          90|         231|      1.89|      473|              14.51|  0.0|0.44|     1.29|                2.75|        0.0|  0.0|     13.73|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:15:22|2023-01-01 02:21:39|2023-01-01 02:22:10|2023-01-01 02:33:14|         125|         246|      2.65|      664|               13.0|  0.0|0.39|     1.15|                2.75|        0.0|  0.0|     13.61|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:26:02|2023-01-01 02:39:09|2023-01-01 02:39:09|2023-01-01 03:03:50|          68|         231|      3.26|     1481|              30.38|  0.0|0.91|      2.7|                2.75|        0.0|  0.0|     28.25|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:09:35|2023-01-01 02:14:11|2023-01-01 02:14:35|2023-01-01 02:49:13|          79|          50|      3.76|     2078|              19.49|  0.0|0.58|     1.73|                2.75|        0.0|  0.0|     23.13|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:42:57|2023-01-01 02:52:03|2023-01-01 02:52:15|2023-01-01 03:31:11|         143|         223|      7.07|     2336|              58.25|  0.0|1.75|     5.17|                2.75|        0.0|13.58|     43.25|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:17:39|2023-01-01 02:23:21|2023-01-01 02:24:48|2023-01-01 02:37:39|          49|         181|      2.15|      771|              13.04|  0.0|0.39|     1.16|                 0.0|        0.0|  3.0|     10.03|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:41:11|2023-01-01 02:45:57|2023-01-01 02:46:20|2023-01-01 02:52:51|         181|          25|      1.09|      391|              19.59|  0.0|0.59|     1.74|                 0.0|        0.0|  0.0|     21.13|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:44:24|2023-01-01 02:53:17|2023-01-01 02:53:40|2023-01-01 03:31:23|          25|         143|     11.08|     2263|              78.77|  0.0|2.36|     6.99|                2.75|        0.0|  0.0|     50.57|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 02:42:03|               null|2023-01-01 02:55:53|2023-01-01 03:26:11|         216|          39|     8.443|     1818|              40.18|  0.0| 1.2|     3.56|                 0.0|        0.0|  0.0|     25.95|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:13:51|2023-01-01 02:26:55|2023-01-01 02:28:05|2023-01-01 02:37:45|         223|           7|      1.65|      580|               9.47|  0.0|0.28|     0.84|                 0.0|        0.0|  0.0|      7.88|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:30:12|2023-01-01 02:38:51|2023-01-01 02:40:51|2023-01-01 02:54:09|           7|         223|      2.02|      798|              21.48|  0.0|0.64|     1.91|                 0.0|        0.0|  0.0|     16.86|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 02:50:41|2023-01-01 02:59:03|2023-01-01 02:59:56|2023-01-01 03:18:47|         223|         145|      3.83|     1131|              18.74|  0.0|0.56|     1.66|                 0.0|        0.0|  0.0|     17.55|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 02:16:47|               null|2023-01-01 02:18:44|2023-01-01 02:48:36|          79|         188|    10.141|     1792|              30.49| 0.98|0.95|     2.82|                2.75|        0.0|  0.0|      27.7|                  N|                N|                 N|               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data = spark.read.parquet(\"data/tlc/fhvhv_tripdata_2023-01.parquet\")\n",
    "trip_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. High Volume Licenses\n",
    "This table contains information about high volume licenses for ride-sharing companies operating in New York City. The table includes license numbers, base names, and affiliated app companies for each license. The table is useful for tracking the licenses of high volume ride-sharing companies operating in NYC and for analyzing their market share in the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------------+-----------+\n",
      "|hv_license_number|license_number|           base_name|affiliation|\n",
      "+-----------------+--------------+--------------------+-----------+\n",
      "|           HV0002|        B02914|     VULCAN CARS LLC|       Juno|\n",
      "|           HV0002|        B02907|        SABO ONE LLC|       Juno|\n",
      "|           HV0002|        B02908|        SABO TWO LLC|       Juno|\n",
      "|           HV0002|        B03035|           OMAHA LLC|       Juno|\n",
      "|           HV0005|        B02510|        TRI-CITY LLC|       Lyft|\n",
      "|           HV0005|        B02844|ENDOR CAR & DRIVE...|       Lyft|\n",
      "|           HV0003|        B02877|        ZWOLF-NY LLC|       Uber|\n",
      "|           HV0003|        B02866|         ZWEI-NY LLC|       Uber|\n",
      "|           HV0003|        B02882|      ZWANZIG-NY LLC|       Uber|\n",
      "|           HV0003|        B02869|         ZEHN-NY LLC|       Uber|\n",
      "|           HV0003|        B02617|          WEITER LLC|       Uber|\n",
      "|           HV0003|        B02876|     VIERZEHN-NY LLC|       Uber|\n",
      "|           HV0003|        B02865|         VIER-NY LLC|       Uber|\n",
      "|           HV0003|        B02512|           UNTER LLC|       Uber|\n",
      "|           HV0003|        B02888|     SIEBZEHN-NY LLC|       Uber|\n",
      "|           HV0003|        B02864|       SIEBEN-NY LLC|       Uber|\n",
      "|           HV0003|        B02883|     SECHZEHN-NY LLC|       Uber|\n",
      "|           HV0003|        B02875|        SECHS-NY LLC|       Uber|\n",
      "|           HV0003|        B02682|       SCHMECKEN LLC|       Uber|\n",
      "|           HV0003|        B02880|     NEUNZEHN-NY LLC|       Uber|\n",
      "+-----------------+--------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hvl_data = spark.read.csv(\"data/tlc/hvl_data.csv\", header= True)\n",
    "hvl_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. zone lookup table\n",
    "Each of the trip records contains a field corresponding to the location of the pickup or drop-off of\n",
    "the trip populated by numbers ranging from 1-263.\n",
    "These numbers correspond to taxi zones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+\n",
      "|location_id|      borough|                zone| service_zone|\n",
      "+-----------+-------------+--------------------+-------------+\n",
      "|          1|          EWR|      Newark Airport|          EWR|\n",
      "|          2|       Queens|         Jamaica Bay|    Boro Zone|\n",
      "|          3|        Bronx|Allerton/Pelham G...|    Boro Zone|\n",
      "|          4|    Manhattan|       Alphabet City|  Yellow Zone|\n",
      "|          5|Staten Island|       Arden Heights|    Boro Zone|\n",
      "|          6|Staten Island|Arrochar/Fort Wad...|    Boro Zone|\n",
      "|          7|       Queens|             Astoria|    Boro Zone|\n",
      "|          8|       Queens|        Astoria Park|    Boro Zone|\n",
      "|          9|       Queens|          Auburndale|    Boro Zone|\n",
      "|         10|       Queens|        Baisley Park|    Boro Zone|\n",
      "|         11|     Brooklyn|          Bath Beach|    Boro Zone|\n",
      "|         12|    Manhattan|        Battery Park|  Yellow Zone|\n",
      "|         13|    Manhattan|   Battery Park City|  Yellow Zone|\n",
      "|         14|     Brooklyn|           Bay Ridge|    Boro Zone|\n",
      "|         15|       Queens|Bay Terrace/Fort ...|    Boro Zone|\n",
      "|         16|       Queens|             Bayside|    Boro Zone|\n",
      "|         17|     Brooklyn|             Bedford|    Boro Zone|\n",
      "|         18|        Bronx|        Bedford Park|    Boro Zone|\n",
      "|         19|       Queens|           Bellerose|    Boro Zone|\n",
      "|         20|        Bronx|             Belmont|    Boro Zone|\n",
      "+-----------+-------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_data= spark.read.csv(\"data/tlc/zone_lookup.csv\", header=True)\n",
    "zone_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data\n",
    "The wunderground data provides information on weather\n",
    "conditions in New York City, including temperature, precipitation, and wind speed.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------+----+----------+---------+--------+------+-------------+----------+\n",
      "|    time|temperature|dew_point|humidity|wind|wind_speed|wind_gust|pressure|precip|    condition|      date|\n",
      "+--------+-----------+---------+--------+----+----------+---------+--------+------+-------------+----------+\n",
      "| 1:51 AM|      47 °F|    26 °F|    44 %| SSW|     8 mph|    0 mph|30.28 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "| 2:51 AM|      46 °F|    26 °F|    46 %| SSW|     7 mph|    0 mph|30.29 in|0.0 in|Partly Cloudy|2023-01-29|\n",
      "| 3:51 AM|      42 °F|    28 °F|    58 %|   S|     5 mph|    0 mph|30.29 in|0.0 in|Partly Cloudy|2023-01-29|\n",
      "| 4:51 AM|      42 °F|    29 °F|    60 %|   S|     6 mph|    0 mph|30.29 in|0.0 in|Partly Cloudy|2023-01-29|\n",
      "| 5:51 AM|      42 °F|    29 °F|    60 %|   S|     5 mph|    0 mph|30.28 in|0.0 in|Partly Cloudy|2023-01-29|\n",
      "| 6:51 AM|      42 °F|    31 °F|    65 %| SSE|     5 mph|    0 mph|30.27 in|0.0 in|Partly Cloudy|2023-01-29|\n",
      "| 7:51 AM|      42 °F|    32 °F|    67 %| SSE|     6 mph|    0 mph|30.26 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "| 8:51 AM|      43 °F|    38 °F|    82 %|   S|     8 mph|    0 mph|30.26 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "| 9:51 AM|      45 °F|    39 °F|    80 %|   S|     9 mph|    0 mph|30.26 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "|10:51 AM|      46 °F|    36 °F|    68 %|   S|     9 mph|    0 mph|30.24 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "|11:51 AM|      49 °F|    33 °F|    54 %| SSW|     9 mph|    0 mph|30.21 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "|12:51 PM|      51 °F|    29 °F|    43 %|   S|     9 mph|    0 mph|30.16 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "| 1:51 PM|      50 °F|    33 °F|    52 %|   S|    12 mph|    0 mph|30.12 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 2:51 PM|      49 °F|    36 °F|    61 %|   S|     9 mph|    0 mph|30.10 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 3:51 PM|      51 °F|    33 °F|    50 %|   S|    10 mph|    0 mph|30.08 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 4:51 PM|      50 °F|    34 °F|    54 %|   S|     8 mph|    0 mph|30.09 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 5:51 PM|      52 °F|    28 °F|    40 %| SSW|    15 mph|   22 mph|30.08 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 6:51 PM|      51 °F|    30 °F|    44 %|  SW|    14 mph|    0 mph|30.09 in|0.0 in|       Cloudy|2023-01-29|\n",
      "| 7:51 PM|      50 °F|    32 °F|    50 %|  SW|    14 mph|    0 mph|30.10 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "| 8:51 PM|      50 °F|    32 °F|    50 %|  SW|     7 mph|    0 mph|30.08 in|0.0 in|Mostly Cloudy|2023-01-29|\n",
      "+--------+-----------+---------+--------+----+----------+---------+--------+------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data = spark.read.csv(\"data/weather/weather.csv\", header=True)\n",
    "weather_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore and Assess the Data\n",
    "Upon exploring the data, I found that there were missing values in some columns and duplicate rows in the TLC data. I also noticed inconsistencies in the date and time formats between the two data sources, which will need to be addressed in the data cleaning process.\n",
    "\n",
    "To clean the data, I will need to remove any duplicate rows, fill in any missing values, and convert the date and time formats to be consistent between the two data sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|request_datetime|on_scene_datetime|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|                0|                   0|             4891992|               0|          4891992|              0|               0|           0|           0|         0|        0|                  0|    0|  0|        0|                   0|          0|   0|         0|                  0|                0|                 0|               0|             0|\n",
      "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_null_counts_exprs = [F.sum(F.col(c).isNull().cast('int')).alias(c) for c in trip_data.columns]\n",
    "trips_null_counts = trip_data.agg(*trips_null_counts_exprs)\n",
    "trips_null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 23:58:40 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "23/04/14 23:58:41 WARN TaskMemoryManager: Failed to allocate a page (60871574 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+-------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee| tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|      weather|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+-------------+\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 06:51:22|2023-01-01 06:59:35|2023-01-01 07:00:06|2023-01-01 07:26:42|         164|           1|     16.92|     1596|              53.14|24.08|2.39|      0.0|                 0.0|        2.5| 7.89|     41.15|                  N|                N|                  |               N|             N|2023-01-01 06|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 07:08:27|2023-01-01 07:09:15|2023-01-01 07:10:55|2023-01-01 07:54:16|         141|           1|      18.1|     2601|              59.95| 21.0| 2.5|      0.0|                 0.0|        2.5|17.19|     47.73|                  N|                N|                  |               N|             N|2023-01-01 07|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 07:22:49|2023-01-01 07:27:41|2023-01-01 07:27:55|2023-01-01 08:03:09|         230|           1|     16.96|     2114|              66.78| 21.0|2.71|      0.0|                 0.0|        2.5|13.94|      45.9|                  N|                N|                  |               N|             N|2023-01-01 07|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 07:33:01|2023-01-01 07:37:40|2023-01-01 07:39:10|2023-01-01 08:26:07|         142|           1|     17.61|     2817|               76.9| 21.0|3.01|      0.0|                 0.0|        2.5|15.51|     58.77|                  N|                N|                  |               N|             N|2023-01-01 07|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 08:03:36|2023-01-01 08:11:48|2023-01-01 08:12:33|2023-01-01 08:44:19|          33|           1|     15.43|     1906|              58.03| 21.0|2.45|      0.0|                 0.0|        2.5| 8.39|     40.49|                  N|                N|                  |               N|             N|2023-01-01 08|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 08:09:41|2023-01-01 08:13:00|2023-01-01 08:13:09|2023-01-01 08:46:53|          50|           1|     15.97|     2024|              55.81| 21.0|2.38|      0.0|                 0.0|        2.5|  0.0|     43.32|                  N|                N|                  |               N|             N|2023-01-01 08|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 08:13:34|2023-01-01 08:23:02|2023-01-01 08:24:23|2023-01-01 08:51:10|         232|           1|     14.24|     1607|              64.29| 21.0|2.63|      0.0|                 0.0|        2.5|  0.0|     51.39|                  N|                N|                  |               N|             N|2023-01-01 08|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 08:19:41|2023-01-01 08:22:42|2023-01-01 08:23:40|2023-01-01 08:56:24|         211|           1|      13.3|     1964|              60.95| 21.0|2.53|      0.0|                 0.0|        2.5| 8.69|     46.98|                  N|                N|                  |               N|             N|2023-01-01 08|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 09:52:27|2023-01-01 09:59:39|2023-01-01 10:00:12|2023-01-01 10:30:05|         186|           1|     14.68|     1793|              49.29| 21.0|2.18|      0.0|                 0.0|        2.5|  0.0|     37.66|                  N|                N|                  |               N|             N|2023-01-01 09|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 10:19:32|2023-01-01 10:26:18|2023-01-01 10:26:51|2023-01-01 10:58:26|          97|           1|     17.54|     1895|              50.26| 21.0|2.21|      0.0|                 0.0|        2.5| 10.0|      41.4|                  N|                N|                  |               N|             N|2023-01-01 10|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 10:19:58|2023-01-01 10:22:50|2023-01-01 10:23:47|2023-01-01 10:52:09|         186|           1|     14.66|     1702|              48.26| 21.0|2.15|      0.0|                 0.0|        2.5|11.08|     36.16|                  N|                N|                  |               N|             N|2023-01-01 10|\n",
      "|           HV0005|              B03406|              B03406|2023-01-01 10:27:17|               null|2023-01-01 10:32:24|2023-01-01 11:07:40|          24|           1|    20.682|     2116|              98.04|22.75| 3.7|      0.0|                 0.0|        2.5|  0.0|     72.93|                  N|                N|                 N|               N|             N|2023-01-01 10|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 10:40:00|2023-01-01 10:27:10|2023-01-01 10:27:47|2023-01-01 10:52:53|          45|           1|      13.4|     1506|               46.0| 21.0|2.09|      0.0|                 0.0|        2.5|  0.0|     38.84|                  N|                N|                  |               N|             N|2023-01-01 10|\n",
      "|           HV0005|              B03406|              B03406|2023-01-01 10:48:45|               null|2023-01-01 10:58:42|2023-01-01 11:28:25|         224|           1|    14.902|     1783|              61.35|20.56|2.53|      0.0|                 0.0|        2.5|  0.0|     36.62|                  N|                N|                 N|               N|             N|2023-01-01 10|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 11:33:19|2023-01-01 11:40:10|2023-01-01 11:42:03|2023-01-01 12:11:30|         164|           1|     14.54|     1767|              55.51| 21.0|2.37|      0.0|                 0.0|        2.5|16.27|     43.31|                  N|                N|                  |               N|             N|2023-01-01 11|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 11:50:05|2023-01-01 11:59:01|2023-01-01 11:59:07|2023-01-01 12:23:23|         164|           1|     14.42|     1456|              44.26| 21.0|2.03|      0.0|                 0.0|        2.5|  0.0|     33.34|                  N|                N|                  |               N|             N|2023-01-01 11|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 12:18:07|2023-01-01 12:24:30|2023-01-01 12:25:55|2023-01-01 12:48:46|         234|           1|     14.07|     1371|              55.13| 21.0|2.36|      0.0|                 0.0|        2.5|  0.0|     34.43|                  N|                N|                  |               N|             N|2023-01-01 12|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 12:51:30|2023-01-01 12:57:56|2023-01-01 12:59:56|2023-01-01 13:37:30|         233|           1|     15.76|     2254|              60.21| 21.0|2.51|      0.0|                 0.0|        2.5|  0.0|     53.94|                  N|                N|                  |               N|             N|2023-01-01 12|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 13:27:51|2023-01-01 13:32:12|2023-01-01 13:32:50|2023-01-01 14:10:03|         243|           1|     23.45|     2233|              69.47| 31.4| 3.1|      0.0|                 0.0|        2.5| 9.89|     54.68|                  N|                N|                  |               N|             N|2023-01-01 13|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 14:54:09|2023-01-01 14:56:34|2023-01-01 14:57:17|2023-01-01 15:32:49|         234|           1|     14.44|     2132|              56.04| 21.0|2.39|      0.0|                 0.0|        2.5|  0.0|     46.09|                  N|                N|                  |               N|             N|2023-01-01 14|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "trip_data = trip_data.drop_duplicates()\n",
    "\n",
    "# fillna in originating_base_num with dispatching_base_num column and add weaher column that can be used to referance weather data\n",
    "trip_data = trip_data.withColumn('originating_base_num', F.coalesce('originating_base_num', 'dispatching_base_num'))\\\n",
    "                        .withColumn('weather', F.date_format(F.from_unixtime((F.unix_timestamp(\"request_datetime\") / 3600) * 3600), \"yyyy-MM-dd HH\")) \n",
    "    \n",
    "trip_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WEATHER DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------+----------+-------------+----------+-------------+\n",
      "|time|temperature|humidity|wind_speed|    condition|      date|   weather_id|\n",
      "+----+-----------+--------+----------+-------------+----------+-------------+\n",
      "|  00|         50|      54|         0|Mostly Cloudy|2023-01-01|2023-01-01 00|\n",
      "|  01|         53|      93|         3|   Light Rain|2023-01-01|2023-01-01 01|\n",
      "|  02|         54|      86|         3|   Light Rain|2023-01-01|2023-01-01 02|\n",
      "|  03|         54|      83|        12|   Light Rain|2023-01-01|2023-01-01 03|\n",
      "|  04|         54|      80|        10|   Light Rain|2023-01-01|2023-01-01 04|\n",
      "|  05|         53|      77|         6|   Light Rain|2023-01-01|2023-01-01 05|\n",
      "|  06|         53|      74|        12|         Fair|2023-01-01|2023-01-01 06|\n",
      "|  07|         52|      77|         9|Partly Cloudy|2023-01-01|2023-01-01 07|\n",
      "|  08|         50|      71|         9|Partly Cloudy|2023-01-01|2023-01-01 08|\n",
      "|  09|         51|      66|        15|Mostly Cloudy|2023-01-01|2023-01-01 09|\n",
      "|  10|         51|      66|        13|Mostly Cloudy|2023-01-01|2023-01-01 10|\n",
      "|  11|         51|      63|        14|Mostly Cloudy|2023-01-01|2023-01-01 11|\n",
      "|  12|         51|      56|        12|Partly Cloudy|2023-01-01|2023-01-01 12|\n",
      "|  13|         52|      53|        18|         Fair|2023-01-01|2023-01-01 13|\n",
      "|  14|         53|      48|        14|         Fair|2023-01-01|2023-01-01 14|\n",
      "|  15|         53|      43|        12|Partly Cloudy|2023-01-01|2023-01-01 15|\n",
      "|  16|         51|      48|        10|         Fair|2023-01-01|2023-01-01 16|\n",
      "|  17|         51|      46|         8|Mostly Cloudy|2023-01-01|2023-01-01 17|\n",
      "|  18|         50|      48|         3|         Fair|2023-01-01|2023-01-01 18|\n",
      "|  19|         50|      48|         0|         Fair|2023-01-01|2023-01-01 19|\n",
      "+----+-----------+--------+----------+-------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def convert_time(time_str):\n",
    "    hour = int(time_str.split(':')[0])\n",
    "    if 'PM' in time_str:\n",
    "        hour += 12\n",
    "    if hour == 12 or hour ==24: # Special case: 12 PM or 12 AM\n",
    "        hour -= 12\n",
    "    return f'{hour:02d}'\n",
    "\n",
    "# Define UDFs\n",
    "convert_time_udf = F.udf(lambda time_str: convert_time(time_str), StringType())\n",
    "split_udf = F.udf(lambda x: int(x.split(' ')[0]), IntegerType())\n",
    "\n",
    "# Change datetime format, make it as a key and apply transformations\n",
    "weather_data = weather_data.withColumn(\"time\", convert_time_udf(\"time\")) \\\n",
    "    .withColumn(\"date\", F.date_format(\"date\", 'yyyy-MM-dd'))\\\n",
    "    .withColumn(\"weather_id\", F.concat_ws(\" \", \"date\", \"time\")) \\\n",
    "    .drop(\"dew_point\", \"wind\", \"wind_gust\", \"pressure\", \"precip\") \\\n",
    "    .withColumn(\"humidity\", split_udf(\"humidity\")) \\\n",
    "    .withColumn(\"wind_speed\", split_udf(\"wind_speed\")) \\\n",
    "    .withColumn(\"temperature\", split_udf(\"temperature\")) \\\n",
    "    .drop_duplicates(['weather_id'])\n",
    "\n",
    "weather_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The Data Model\n",
    "For this project, I have decided to use star schema. With a star schema based on the TLC Trip Record Data  and weather data, \n",
    "you can perform a variety of analytical queries to gain insights into for-hire vehicle industry in New York City.\n",
    "\n",
    "##### Schema contain one fact table and four Dimension Tables:\n",
    "- Fact Table: \n",
    "    - Trips:\n",
    "        trip_id (Primary key)\n",
    "\n",
    "        hvfhs_license_num (Foreign key referencing HVFHS)\n",
    "\n",
    "        dispatching_base_num (Foreign key referencing base_num)\n",
    "\n",
    "        originating_base_num (Foreign key referencing base_num)\n",
    "\n",
    "        request_datetime (Foreign key referencing DateTimes)\n",
    "\n",
    "        pickup_datetime (Foreign key referencing DateTimes)\n",
    "\n",
    "        dropoff_datetime (Foreign key referencing DateTimes)\n",
    "\n",
    "        PULocationID (Foreign key referencing Locations)\n",
    "\n",
    "        DOLocationID (Foreign key referencing Locations)\n",
    "\n",
    "        weather,\n",
    "        trip_miles,\n",
    "        trip_time,\n",
    "        base_passenger_fare,\n",
    "        tolls,\n",
    "        bcf,\n",
    "        sales_tax,\n",
    "        congestion_surcharge,\n",
    "        airport_fee,\n",
    "        tips,\n",
    "        driver_pay,\n",
    "        shared_request_flag,\n",
    "        shared_match_flag\n",
    "\n",
    "- Dimension Tables:\n",
    "    - HVFHS :\n",
    "        hv_license_num (Primary key), \n",
    "        base_num (Primary key),\n",
    "        Affiliation,\n",
    "        base_name,\n",
    "\n",
    "    - DateTimes: \n",
    "        datetime_id (Primary key)\n",
    "        full_datetime\n",
    "        date\n",
    "        hour\n",
    "        day\n",
    "        month\n",
    "        week\n",
    "        day_of_week\n",
    "\n",
    "    - Locations: \n",
    "        location_id (Primary key), \n",
    "        borough, \n",
    "        zone, \n",
    "        service_zone\n",
    "    \n",
    "    - weather: \n",
    "        weather_id,\t\n",
    "        date,\t\n",
    "        time,\t\n",
    "        temperature,\t\t\n",
    "        humidity, \t\n",
    "        wind_speed,\t\t\t\n",
    "        condition\n",
    "\n",
    "I chose this model because it allows for easy querying and analysis of the data. To pipeline the data into this model, I will need to use ETL (Extract, Transform, Load) processes to extract the data from the original sources, transform the data to match the desired data model, and load the transformed data into the appropriate tables in a relational database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run ETL to Model the Data\n",
    "To create the data pipelines and the data model, I will use Python and PySpark. I will write Python scripts to extract and transform the data and load the transformed data into the appropriate data lake on S3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Complete Project Write Up\n",
    "The goal of this project is to create an analytics datalake that can be used to analyze the relationship between weather conditions and taxi trips in New York City. Some example queries that could be run on this data lake such as:\n",
    "\n",
    "        - What is the average fare for taxi trips on rainy days versus sunny days?\n",
    "        - Get the total revenue for each weather condition:\n",
    "        - How does the number of taxi trips vary with temperature?\n",
    "        - Get the average trip distance and time for each weather condition\n",
    "\n",
    "Airflow could be incorporated into this project to manage the ETL workflows and automate the data pipeline processes.\n",
    "\n",
    "I chose a star data model for this project because it allows for easy querying and analysis of the data. If the data was increased by 100x, I would consider using a distributed computing system, such as AWS EMR, to handle the increased data volume.\n",
    "\n",
    "If the pipelines were run on a daily basis by 7am, I would use Airflow to schedule and automate the ETL workflows.\n",
    "\n",
    "If the database needed to be accessed by 100+ people, I would consider using a cloud-based solution, such as Amazon Redshift or Google BigQuery, to ensure scalability and availability. I would also implement security measures, such as role-based access control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
